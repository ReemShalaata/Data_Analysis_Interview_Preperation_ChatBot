{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "openai_api_key  = os.getenv('OPENAI_API_KEY')\n",
    "client=OpenAI()\n",
    "dataset = pd.read_excel('data_analysis_interview__qa.xlsx')  # Assuming it's an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Technical', 'Easy')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_user_choices():\n",
    "    # Define the available categories and difficulties in lowercase\n",
    "    categories = ['Technical', 'Behavioral']\n",
    "    difficulties = ['Easy', 'Medium', 'Hard']\n",
    "\n",
    "    while True:\n",
    "        chosen_category = input(\"Enter question's category [Technical, Behavioral]: \").strip().capitalize()\n",
    "        if chosen_category in categories:\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Invalid category. Please choose from {categories}.\")\n",
    "\n",
    "    while True:\n",
    "        chosen_difficulty = input(\"Enter question's difficulty [Easy, Medium, Hard]: \").strip().capitalize()\n",
    "        if chosen_difficulty in difficulties:\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Invalid difficulty. Please choose from {difficulties}.\")\n",
    "\n",
    "    return chosen_category.capitalize(), chosen_difficulty\n",
    "\n",
    "get_user_choices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_question(category,difficulty_level):\n",
    "    filtered_dataset = dataset[(dataset['Category']==category) & (dataset['Difficulty'] == difficulty_level)] \n",
    "    single_data_point=filtered_dataset.sample().iloc[0]\n",
    "    question,answer=single_data_point['Question'],single_data_point['Answer']\n",
    "    return question,answer\n",
    "# suggest_question(category='Technical',difficulty_level='Easy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Question': 'What is the purpose of A/B testing in data analysis?',\n",
      " 'Answer': 'A/B testing is used to compare two versions of a variable (like a webpage) to determine which one performs better in a controlled experiment.',\n",
      " 'Category': 'Technical',\n",
      " 'Difficulty': 'Medium'}\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'dataset' is your DataFrame or data structure\n",
    "\n",
    "# Create a new question prompt\n",
    "new_question_prompt = (\n",
    "    \"As a data analysis interviewer, generate a data analysis interview question and provide its answer. \"\n",
    "    \"The answer should be at lest 30 words\"\n",
    "    \"The output should explicitly include the interviewer's question and the corresponding answer as a dictionary. \"\n",
    "    \"The dictionary should also include the difficulty of the question ['Technical', 'Behavioral']. \"\n",
    "    \"The dictionary should also include the category of the question ['Easy', 'Medium', 'Hard']. \"\n",
    "    \"The dictionary must have only four keys ['Question', 'Answer', 'Category', 'Difficulty'] with the following pattern: \"\n",
    "    \"{'Question': generated question, 'Answer': generated answer, 'Category': question's category, 'Difficulty': question's difficulty}.\\n\"\n",
    ")\n",
    "\n",
    "prompt_examples = \"\"\n",
    "\n",
    "# Loop through the dataset\n",
    "for index, row in dataset.iterrows():\n",
    "    question = row['Question']\n",
    "    answer = row['Answer']\n",
    "    category = row['Category']\n",
    "    difficulty = row['Difficulty']\n",
    "    prompt_examples += (f\"Question: {question}\\nAnswer: {answer}\\nCategory: {category}\\nDifficulty: {difficulty}\\n\\n\")\n",
    "\n",
    "# Append the new question prompt\n",
    "prompt_examples += new_question_prompt\n",
    "\n",
    "# Send the prompt to the GPT-3.5 model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    messages=[{'role': 'user', 'content': prompt_examples}],\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Extract and print the response content\n",
    "response_text = response.choices[0].message.content\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Question': 'What is the purpose of A/B testing in data analysis?', 'Answer': 'A/B testing is used to compare two versions of a variable (like a webpage) to determine which one performs better in a controlled experiment.', 'Category': 'Technical', 'Difficulty': 'Medium'}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Convert the string representation of a dictionary to an actual dictionary\n",
    "response_dict = ast.literal_eval(response_text)\n",
    "\n",
    "# Now you can access the data in the dictionary\n",
    "print(response_dict)  # Output: What is data analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the purpose of A/B testing in data analysis?\n",
      "Your answer was: I don't know what is that\n"
     ]
    }
   ],
   "source": [
    "generated_question=response_dict['Question']\n",
    "expected_answer=response_dict['Answer']\n",
    "print(f\"Question: {generated_question}\")\n",
    "\n",
    "\n",
    "# Prompt the user to input their answer\n",
    "user_answer = input(\"Your Answer: \")\n",
    "\n",
    "# Optional: Process the user's response\n",
    "# For example, you can compare it with the generated answer, or store it for later use\n",
    "print(\"Your answer was:\", user_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# Function to encode text to vector using SentenceTransformer\n",
    "def encode(text):\n",
    "    return model.encode(text, convert_to_tensor=True,device='cuda')\n",
    "def evaluate_answer(expected_answer,user_answer):\n",
    "    # Encoding\n",
    "    expected_answer_vec = encode(expected_answer).unsqueeze(0).cpu()\n",
    "    user_answer_vec = encode(user_answer).unsqueeze(0).cpu()\n",
    "\n",
    "    similarity = cosine_similarity(expected_answer_vec, user_answer_vec)\n",
    "\n",
    "    print(f\"Expected Answer: {user_answer}\")\n",
    "    print(f\"User's Answer: {expected_answer}\")\n",
    "    print(f\"Cosine similarity score: {similarity[0][0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Answer: I don't know what is that\n",
      "User's Answer: A/B testing is used to compare two versions of a variable (like a webpage) to determine which one performs better in a controlled experiment.\n",
      "Cosine similarity score: 0.13407671451568604\n"
     ]
    }
   ],
   "source": [
    "evaluate_answer(expected_answer,user_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user's answer is inaccurate. The capital of France is Paris, not Lyon. The user should review their knowledge of French geography to provide correct answers.\n"
     ]
    }
   ],
   "source": [
    "def get_feedback_from_gpt(generated_question, user_answer, expected_answer):\n",
    "    prompt = (f\"Question: {generated_question}\\n\"\n",
    "              f\"Expected Answer: {expected_answer}\\n\"\n",
    "              f\"User's Answer: {user_answer}\\n\\n\"\n",
    "              \"Provide detailed feedback on the user's answer for the generated quesion, comparing it with the expected answer and pointing out\"\n",
    "              \"any inaccuracies, areas of improvement, or aspects that are well addressed.\"\n",
    "              \"The output feedback should not be more than 50 words\")\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        max_tokens=150\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "generated_question = \"What is the capital of France?\"\n",
    "user_answer = \"The capital of France is Lyon.\"\n",
    "expected_answer = \"The capital of France is Paris.\"\n",
    "feedback = get_feedback_from_gpt(generated_question, user_answer, expected_answer)\n",
    "print(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
